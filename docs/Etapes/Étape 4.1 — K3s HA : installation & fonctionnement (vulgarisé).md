# Ã‰tape 4.1 â€” K3s HA : installation & fonctionnement (vulgarisÃ©) ğŸ§±ğŸ“

> Objectif : expliquer **ce qui a Ã©tÃ© installÃ©**, **pourquoi**, et **comment Ã§a fonctionne**, afin de consolider des bases Kubernetes avec K3s (Kubernetes â€œlÃ©gerâ€). [web:273]

---

## 1) Contexte : pourquoi K3s ? ğŸ¯

K3s est une distribution Kubernetes **simplifiÃ©e** :
- un binaire principal,
- moins de dÃ©pendances,
- des composants â€œpackagÃ©sâ€ (packaged components = addons fournis et gÃ©rÃ©s par K3s). [web:265]

Dans ce projet, K3s sert de **socle** pour une plateforme DevSecOps (outils + SSO + sÃ©curitÃ©), donc le cluster doit Ãªtre :
- stable,
- reproductible,
- suffisamment HA pour un environnement â€œsÃ©rieuxâ€ (mÃªme en lab). [web:104]

---

## 2) Architecture installÃ©e : HA avec embedded etcd ğŸ§ ğŸ—„ï¸

### RÃ´les des nÅ“uds
- **Masters** (control plane) : `master1`, `master2`, `master3`
  - hÃ©bergent lâ€™API Kubernetes (API server = point dâ€™entrÃ©e du cluster),
  - hÃ©bergent aussi **etcd** (base de donnÃ©es distribuÃ©e du cluster). [web:104]
- **Worker** : `worker1`
  - exÃ©cute principalement les applications (pods). [web:104]

### Pourquoi 3 masters ?
Le mode HA â€œembedded etcdâ€ repose sur etcd qui fonctionne mieux avec un nombre impair de nÅ“uds (quorum = majoritÃ© nÃ©cessaire pour valider des Ã©critures). [web:104]

---

## 3) Ce que â€œkubectl get nodesâ€ prouve âœ…

La commande `kubectl get nodes -o wide` montre :
- `STATUS Ready` : le nÅ“ud est joignable et fonctionne correctement.
- `ROLES control-plane,etcd` sur les masters : HA control plane + datastore etcd actifs. [web:104]
- `INTERNAL-IP 192.168.56.101-104` : les nÅ“uds communiquent sur le rÃ©seau privÃ© Vagrant (host-only), donc stable et prÃ©visible.

---

## 4) RÃ©seau Kubernetes : pourquoi les pods ont des IP en 10.42.0.x ğŸŒ

Les pods reÃ§oivent des IP â€œinternes clusterâ€ (overlay = rÃ©seau virtuel au-dessus du rÃ©seau des VM).
Dans ton cas :
- pods en `10.42.0.x` (ex: CoreDNS `10.42.0.4`)  
Cela confirme que le rÃ©seau pods est fonctionnel (les pods peuvent se parler via ce rÃ©seau). [web:251]

> Note : K3s utilise souvent Flannel comme CNI (CNI = plugin rÃ©seau Kubernetes). Selon la configuration, tous les composants ne sâ€™observent pas toujours sous forme de pods dÃ©diÃ©s, mais le fait que CoreDNS tourne et a une IP pods est un indicateur clÃ©. [web:251][web:252]

---

## 5) Les composants systÃ¨me observÃ©s (kube-system) ğŸ§©

Les pods â€œkube-systemâ€ sont des briques de base.

### 5.1 CoreDNS (DNS du cluster) ğŸ“¡
- RÃ´le : rÃ©soudre des noms de services/pods Ã  lâ€™intÃ©rieur du cluster.
- Preuve : `kube-dns` a des endpoints (`kubectl get endpoints -n kube-system`). [web:251]

Sans DNS interne, beaucoup dâ€™applications Kubernetes â€œne tiennent pasâ€ (services qui ne se trouvent pas).

### 5.2 local-path-provisioner (stockage simple) ğŸ’¾
- RÃ´le : fournir des volumes persistants simples pour un lab.
- UtilitÃ© : permet Ã  une application de stocker des donnÃ©es sans systÃ¨me de stockage distribuÃ©.

> Plus tard dans le projet : Longhorn remplacera ce modÃ¨le pour du stockage distribuÃ© (Phase 2). (stockage distribuÃ© = volumes rÃ©pliquÃ©s sur plusieurs nÅ“uds)

### 5.3 metrics-server (mÃ©triques Kubernetes) ğŸ“Š
- RÃ´le : fournir des mÃ©triques de base (CPU/RAM) aux API Kubernetes.
- UtilitÃ© : nÃ©cessaire pour `kubectl top`, et utile pour la supervision (plus tard Prometheus/Grafana). [web:265]

---

## 6) Pourquoi Traefik nâ€™apparaÃ®t pas (câ€™est voulu) ğŸš¦
K3s peut fournir Traefik â€œpackagÃ©â€ par dÃ©faut, mais dans ce projet Traefik a Ã©tÃ© **dÃ©sactivÃ©** dans la configuration K3s. [web:265]

But :
- garder le contrÃ´le sur lâ€™Ingress (Ingress controller = point dâ€™entrÃ©e HTTP/HTTPS),
- installer Traefik ensuite avec une configuration propre (TLS, middlewares, sÃ©curitÃ©) Ã  lâ€™Ã‰tape 5.

La commande :
```bash
kubectl -n kube-system get pods | egrep -i 'traefik|helm|svclb'

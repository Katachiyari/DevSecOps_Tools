# √âtape 4.1 ‚Äî K3s HA : installation & fonctionnement (vulgaris√©) üß±üéì

> Objectif : expliquer **ce qui a √©t√© install√©**, **pourquoi**, et **comment √ßa fonctionne**, afin de consolider des bases Kubernetes avec K3s (Kubernetes ‚Äúl√©ger‚Äù). [web:273]

---

## 1) Contexte : pourquoi K3s ? üéØ

K3s est une distribution Kubernetes **simplifi√©e** :
- un binaire principal,
- moins de d√©pendances,
- des composants ‚Äúpackag√©s‚Äù (packaged components = addons fournis et g√©r√©s par K3s). [web:265]

Dans ce projet, K3s sert de **socle** pour une plateforme DevSecOps (outils + SSO + s√©curit√©), donc le cluster doit √™tre :
- stable,
- reproductible,
- suffisamment HA pour un environnement ‚Äús√©rieux‚Äù (m√™me en lab). [web:104]

---

## 2) Architecture install√©e : HA avec embedded etcd üß†üóÑÔ∏è

### R√¥les des n≈ìuds
- **Masters** (control plane) : `master1`, `master2`, `master3`
  - h√©bergent l‚ÄôAPI Kubernetes (API server = point d‚Äôentr√©e du cluster),
  - h√©bergent aussi **etcd** (base de donn√©es distribu√©e du cluster). [web:104]
- **Worker** : `worker1`
  - ex√©cute principalement les applications (pods). [web:104]

### Pourquoi 3 masters ?
Le mode HA ‚Äúembedded etcd‚Äù repose sur etcd qui fonctionne mieux avec un nombre impair de n≈ìuds (quorum = majorit√© n√©cessaire pour valider des √©critures). [web:104]

---

## 3) Ce que ‚Äúkubectl get nodes‚Äù prouve ‚úÖ

La commande `kubectl get nodes -o wide` montre :
- `STATUS Ready` : le n≈ìud est joignable et fonctionne correctement.
- `ROLES control-plane,etcd` sur les masters : HA control plane + datastore etcd actifs. [web:104]
- `INTERNAL-IP 192.168.56.101-104` : les n≈ìuds communiquent sur le r√©seau priv√© Vagrant (host-only), donc stable et pr√©visible.

---

## 4) R√©seau Kubernetes : pourquoi les pods ont des IP en 10.42.0.x üåê

Les pods re√ßoivent des IP ‚Äúinternes cluster‚Äù (overlay = r√©seau virtuel au-dessus du r√©seau des VM).
Dans ton cas :
- pods en `10.42.0.x` (ex: CoreDNS `10.42.0.4`)  
Cela confirme que le r√©seau pods est fonctionnel (les pods peuvent se parler via ce r√©seau). [web:251]

> Note : K3s utilise souvent Flannel comme CNI (CNI = plugin r√©seau Kubernetes). Selon la configuration, tous les composants ne s‚Äôobservent pas toujours sous forme de pods d√©di√©s, mais le fait que CoreDNS tourne et a une IP pods est un indicateur cl√©. [web:251][web:252]

---

## 5) Les composants syst√®me observ√©s (kube-system) üß©

Les pods ‚Äúkube-system‚Äù sont des briques de base.

### 5.1 CoreDNS (DNS du cluster) üì°
- R√¥le : r√©soudre des noms de services/pods √† l‚Äôint√©rieur du cluster.
- Preuve : `kube-dns` a des endpoints (`kubectl get endpoints -n kube-system`). [web:251]

Sans DNS interne, beaucoup d‚Äôapplications Kubernetes ‚Äúne tiennent pas‚Äù (services qui ne se trouvent pas).

### 5.2 local-path-provisioner (stockage simple) üíæ
- R√¥le : fournir des volumes persistants simples pour un lab.
- Utilit√© : permet √† une application de stocker des donn√©es sans syst√®me de stockage distribu√©.

> Plus tard dans le projet : Longhorn remplacera ce mod√®le pour du stockage distribu√© (Phase 2). (stockage distribu√© = volumes r√©pliqu√©s sur plusieurs n≈ìuds)

### 5.3 metrics-server (m√©triques Kubernetes) üìä
- R√¥le : fournir des m√©triques de base (CPU/RAM) aux API Kubernetes.
- Utilit√© : n√©cessaire pour `kubectl top`, et utile pour la supervision (plus tard Prometheus/Grafana). [web:265]

---

## 6) Pourquoi Traefik n‚Äôappara√Æt pas (c‚Äôest voulu) üö¶
K3s peut fournir Traefik ‚Äúpackag√©‚Äù par d√©faut, mais dans ce projet Traefik a √©t√© **d√©sactiv√©** dans la configuration K3s. [web:265]

But :
- garder le contr√¥le sur l‚ÄôIngress (Ingress controller = point d‚Äôentr√©e HTTP/HTTPS),
- installer Traefik ensuite avec une configuration propre (TLS, middlewares, s√©curit√©) √† l‚Äô√âtape 5.

La commande :
```bash
kubectl -n kube-system get pods | egrep -i 'traefik|helm|svclb'
La commande ne retourne rien ‚Üí coh√©rent avec l‚Äôobjectif.

---

## 7) Pr√©-requis OS : pourquoi swap=OFF et sysctl=1

Kubernetes/K3s attend un syst√®me qui route correctement le trafic r√©seau inter-pods :

- `net.ipv4.ip_forward=1` (forwarding = routage IP)
- `net.bridge.bridge-nf-call-iptables=1` (iptables = filtrage/r√®gles r√©seau)
- swap d√©sactiv√© (swap = RAM sur disque, peut perturber les garanties de ressources) [web:82]

Ces r√©glages augmentent la stabilit√© du cluster.

---

## 8) R√©sum√© : ce qui est acquis √† la fin de l‚Äô√âtape 4

- Un cluster K3s HA fonctionnel :
  - 3 masters (control plane + etcd)
  - 1 worker
- Un DNS interne op√©rationnel (CoreDNS).
- Un r√©seau pods fonctionnel (IP pods en 10.42.0.x).
- Un stockage simple disponible (local-path provisioner).
- Une base de m√©triques disponible (metrics-server).
- Ingress non install√© volontairement (Traefik packag√© d√©sactiv√©). [web:104][web:265]

---

## 9) Prochaine √©tape (√âtape 5)

Mettre en place l‚Äôacc√®s HTTP/HTTPS aux applications :

- Traefik (Ingress controller)
- cert-manager (certificats TLS)
- Une application ‚Äúhello world‚Äù expos√©e en HTTPS
